---
tags:
  - СистемныйАнализ
aliases:
  - Энтропия дискретных источников сообщений и сложных систем
---

В определении "количества информации"  существуют 2 основных подхода: вероятностный и объемный.

## Вероятностный подход (по К. Шеннону)
Для начальной иллюстрации вероятностного подхода рассмотрим пример с бросанием игральной кости с $N$ гранями (как правило,N = 6). Результатом опыта является выпадение грани с одним из $N$ знаков с априорной вероятностью, равной $1/N$.

Введем в рассмотрение меру неопределенности исхода опыта —**энтропию** и обозначим ее символом $Н$. Неопределенность, естественно, тем выше, чем больше возможных вариантов исхода опыта. Очевидно, что величины $N$ и $Н$ связаны некоторой функциональной зависимостью: 
$$Н = f (N)$$
причем функция $f$ является неотрицательной и возрастающей по мере увеличения количества возможных исходов. 
Проследим за результатом однократного бросания кости:
1. перед бросанием кости исход опыта неизвестен, априорную энтропию (степень неопределенности) обозначим $Н_1$ ;
2. кость брошена — информация об исходе опыта получена, обозначим количество этой информации через $I$;
3. остаточную неопределенность (энтропию) опыта (после его проведения) обозначим через $Н_2$ 


Количество информации, полученное в результате проведения опыта, определим, как разность между априорной и апостериорной неопределенностью:
$$I = Н_1 – Н_2$$
Очевидно, что при получении конкретного результата имевшаяся до опыта неопределенность оказывается полностью снятой т. е .$Н_2 = 0$, и количество полученной информации совпадает с априорной энтропией. 

Теперь следует определить вид функции $f$. Если варьируется количество вариантов исхода опыта $N$ и число бросаний кости $М$, то общее количество возможных исходов будет равно $$Х = N^M$$Следовательно, вероятность верного предсказания исходов опыта с увеличением числа бросаний кости уменьшается по степенному закону. Например, при двух бросаниях шестигранной кости $Х = 6^2 = 36$, и вероятность угадать оба исхода равна $1/36$. Серия опытов с $М$ бросаниями игральной кости состоит из $М$ независимых равновероятных событий. Энтропия (неопределенность прогноза) исхода такой серии в $М$ раз больше, чем энтропия единичного опыта. 
### Принцип аддитивности (суммируемости) энтропии. 
Этот принцип состоит в том, что энтропия сложной системы(в том числе дискретного сообщения определенной длительности) равна сумме энтропий ее подсистем. 

Как видим, энтропию можно рассматривать как меру неопределенности источника сообщений. При вероятностном подходе ожидаемое состояние источника сообщения характеризуется неопределенностью, которая снимается (полностью или частично) после получения сообщения. Поэтому получаемая информация на один символ, переданный источником, количественно определяет степень уменьшения неопределенности. 

### Формула Хартли
Для достаточно простого случая выбора из $N$ дискретных равновероятных событий проведенные рассуждения приводят к выводу **меры Хартли** $$Н = log_2N$$В принципе, выбор основания логарифма связан с понятием алфавита, выбранного для представления информации. Формула выше записана для двоичного алфавита. Для перехода от одного алфавита к другому перед логарифмом вводится безразмерный множитель, чтобы само количество информации оставалось неизменным. 

>[!INFO]
>По формуле Хартли за единицу информации принимается количество информации, которую несет исход опыта по выбору одного из двух независимых равновероятных событий. 
>

Например - бросание монеты, при котором возможно только два исхода

В более сложном и практически важном случае не равновероятных исходов (событий), например, передачи сообщений с помощью символов алфавита количество информации, равное количеству снятой энтропии, вычисляется по формуле Шеннона
### Формула Шеннона
Формула Шеннона используется для расчета энтропии или количества информации в системе, учитывая вероятности различных событий. Формула Шеннона выглядит следующим образом:

$$H = -\sum_{i=1}^N P_i \log_2(P_i)$$

Где:
- $H$ - энтропия или количество информации в системе (в битах).
- $N$ - общее количество возможных событий.
- $P_i$ - вероятность появления события $i$.

Эта формула позволяет вычислить, сколько бит информации содержится в системе, учитывая вероятности различных событий. Большие значения энтропии указывают на большую неопределенность или разнообразие событий в системе, в то время как меньшие значения указывают на более предсказуемую систему.

(Клименко, И. С. Системный анализ в управлении : учебное пособие для вузов / И. С. Клименко. — 2е изд., стер. — СанктПетербург : Лань, 2021. — ISBN 9785811469420. — Текст : электронный // Лань : электроннобиблиотечная система. — URL: https://e.lanbook.com/book/153690 (дата обращения: 08.09.2023). — Режим доступа: для авториз. пользователей. — С. 37.).
